{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo code\n",
    "\n",
    "---- PREPARE DATA BY SPLITTING INTO INCREMENTAL LOAD\n",
    "\n",
    "1. Convert to date data type\n",
    "if yy > 25 then:\n",
    "    Custom function to convert 20th century years dates to 1997 instead of 2097 using strftime\n",
    "else:\n",
    "    to_date\n",
    "\n",
    "2. Function to iteratively for count distinct yyyy-mm months:\n",
    "- dt = every date\n",
    "- append 1 day's data to df\n",
    "- save as '/date/csv_name' + dt + '.csv'\n",
    "- dateadd a day\n",
    "\n",
    "---- CONVERT EACH SAVED CSV FILE TO PARQUET AND SAVE IT TO 'LANDING' FOLDER\n",
    "\n",
    "---- BRONZE LAYER: ConformInterface\n",
    "1. Read parquet file from LANDING for a specific date and a schema.json\n",
    "2. Write a pyspark code to verify parquet schema vs mentioned schema in json.\n",
    "3. Move parquet file to BRONZE if verification is successful else throw error and stop processing.\n",
    "\n",
    "---- SILVER LAYER: StandardizeColumns\n",
    "1. Read parquet file from BRONZE for same specific date\n",
    "2. Add audit columns to spark dataframe\n",
    "3. Save dataframe to SILVER with same name and delete file in BRONZE\n",
    "\n",
    "---- GOLD LAYER: ChangeDataCapture\n",
    "1. Perform incremental load of only changed data\n",
    "\n",
    "---- MODEL SQL DATABASE SCHEMA, IT WILL ACT AS WAREHOUSE (EXPLORE OPTIONS FOR FREE TIER)    -dbdiagram.io, erdlab.io\n",
    "\n",
    "---- COPY GOLD DATA INTO SQL WAREHOUSE\n",
    "\n",
    "---- USE DBT FREE TIER TO INTEGRATE, ANY TRANSFORMATION, CREATE EXTRACT TABLES\n",
    "\n",
    "---- SERVE EXTRACT TABLE TO SERVING FOLDER AS CSV FOR ANALYSIS\n",
    "\n",
    "---- CREATE POWERBI DASHBOARD\n",
    "\n",
    "---- CREATE ML MODEL TO PREDICT FUTURE PRICES BASED ON DOMESTIC FACTORS SUCH AS MSP, PRODUCTION\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rice_path = 'datasets/daily_retail_price_Rice_upto-apr_2015.csv'\n",
    "df = spark.read.csv(rice_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------+-----+\n",
      "|    Date|Centre_Name|Commodity_Name|Price|\n",
      "+--------+-----------+--------------+-----+\n",
      "|25-11-97|      DELHI|          Rice|   10|\n",
      "|25-11-97|     SHIMLA|          Rice|   12|\n",
      "|25-11-97|    LUCKNOW|          Rice|  6.5|\n",
      "|25-11-97|  AHMEDABAD|          Rice|   10|\n",
      "|25-11-97|     BHOPAL|          Rice|    9|\n",
      "+--------+-----------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupBy('Centre_Name').agg(countDistinct('Date')).alias('no_of_dates')\n",
    "df1.toPandas().to_csv('locations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string type date field to date type and yyyy-mm-dd format\n",
    "\n",
    "df = df.withColumn(\"date_column\", to_date(df[\"Date\"], \"dd-MM-yy\"))\n",
    "--year = year-100 if year>2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an identity column\n",
    "\n",
    "df = df.coalesce(1).withColumn(\"idx\", monotonically_increasing_id())\n",
    "w = W.orderBy(\"idx\")\n",
    "df = df.withColumn(\"id\", F.row_number().over(w))\n",
    "df = df.drop(\"idx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------+-----+-----------+---+\n",
      "|    Date|Centre_Name|Commodity_Name|Price|date_column| id|\n",
      "+--------+-----------+--------------+-----+-----------+---+\n",
      "|25-11-97|      DELHI|          Rice|   10| 2097-11-25|  1|\n",
      "|25-11-97|     SHIMLA|          Rice|   12| 2097-11-25|  2|\n",
      "|25-11-97|    LUCKNOW|          Rice|  6.5| 2097-11-25|  3|\n",
      "|25-11-97|  AHMEDABAD|          Rice|   10| 2097-11-25|  4|\n",
      "|25-11-97|     BHOPAL|          Rice|    9| 2097-11-25|  5|\n",
      "+--------+-----------+--------------+-----+-----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------+-----+-----------+\n",
      "|    Date|Centre_Name|Commodity_Name|Price|date_column|\n",
      "+--------+-----------+--------------+-----+-----------+\n",
      "|25-11-97|      DELHI|          Rice|   10| 2097-11-25|\n",
      "|25-11-97|     SHIMLA|          Rice|   12| 2097-11-25|\n",
      "|25-11-97|    LUCKNOW|          Rice|  6.5| 2097-11-25|\n",
      "|25-11-97|  AHMEDABAD|          Rice|   10| 2097-11-25|\n",
      "|25-11-97|     BHOPAL|          Rice|    9| 2097-11-25|\n",
      "+--------+-----------+--------------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(df.id).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------+-----+-----------+---+\n",
      "|    Date|Centre_Name|Commodity_Name|Price|date_column| id|\n",
      "+--------+-----------+--------------+-----+-----------+---+\n",
      "|25-11-97|      DELHI|          Rice|   10| 2097-11-25|  0|\n",
      "|25-11-97|     SHIMLA|          Rice|   12| 2097-11-25|  1|\n",
      "|25-11-97|    LUCKNOW|          Rice|  6.5| 2097-11-25|  2|\n",
      "|25-11-97|  AHMEDABAD|          Rice|   10| 2097-11-25|  3|\n",
      "|25-11-97|     BHOPAL|          Rice|    9| 2097-11-25|  4|\n",
      "+--------+-----------+--------------+-----+-----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suddata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
